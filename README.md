# Data Engineering Project
# NYC Citi Bike Sharing Optimization Application
Data Pipeline For Bike Sharing Data using Apache Airflow, Redshift, and Spark

## Use Case of This Project

Bike and scooter sharing services have taken the world by storm in the past few years. It is a service that has companies, governments, and people interested in moving our cities and communities into a more sustainable enviroments. The aim of many stakeholders is to encourage everyone to get to their destinations utilizing bikes and electric scooters. This nobel innovation is a service to our earth to fight global warming, to our society to be more enviromentally conscious, and to our communities to be more sustainable. Unfortunately, bike and scooter sharing have faced dangerous problems with cities, like New York City, not ready to provide safe routes or zoned parking. Other problems occur on accessibility and optimization of providers of this service like Uber, Lyft, and Citi Bikes. I believe that a unified source of bike data from providers, government, and users is an important step to allow us to start solving these problems. This project will serve as a first step to create mentioned data source.

## Database Setup

We have two data sources that we are working with: trips from citi bike in NYC & bike route data from NYC local government.

The trips from citi bike in NYC data are located as zip files in an S3 bucket partitioned by month. 

And the bike route data is retrieved through an api [endpoint](https://data.cityofnewyork.us/resource/cc5c-sm6z.json).

Based on the datasets mentioned and that we need to load data in row form for potentially later use cases of the data, we need to create a data lake for our raw data files with two fact tables in a galaxy schema. The fact tables would represent trips and routes data. While our dimension tables would be divided to stations, streets, and time, that hold information regarding location and time data of different variables in our fact tables. Here is the following table structures represented in the diagram below:

![](img/DataModel.png)

2. AWS credentials and Redshift Connections must be configured in Airflow

## Elements of The Project

The following python scripts were used to develop this pipeline:

1. de_dag.py: Defines the dag and assigns operators to each respective task in the Airflow pipeline. Additionally, it dictates the sequence of tasks at the bottom of the file.

2. sql_queries.py: Defines SQL queries to read and load staging, fact, and dim tables. Found in plugins/helpers directory

3. stage_redshift.py: Defines StageToRedshiftOperator that reads from S3 and writes to staging tables in AWS Redshfit

4. load_fact.py: Defines LoadFactOperator that extracts data from staging into songplays (or otherwise defined) fact table

5. load_dimension.py: Defines LoadDimensionOperator that extracts data from staging into the song, user, artist, and time dim tables

6. data_quality.py: Defines DataQualityOperator that counts the rows in each table to ensure data has been written to RedShift

## Using The Project

1. Check links of the data provided in the S3 section of the dwh.cfg file. The song dataset is a subsection of sets from [The Million Song Dataset](http://millionsongdataset.com/), and the user log dataset is generated by this [event simulator](https://github.com/Interana/eventsim).

2. Run create_tables.sql file in order to create the staging, dimension, and fact tables in RedShift.

3. Configure your AWS credentials and RedShift connections in Airflow, check [here](https://towardsdatascience.com/how-to-deploy-airflow-on-aws-best-practices-63778d6eab9e) for more info on the topic.

Note: You need to make sure that Pandas and JSON liberaries are installed.*